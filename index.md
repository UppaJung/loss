---
layout: layouts/index.vto
bodyClass: body-index
templateEngine: [vto, md]
date: Git Last Modified
---

<div style="display: block; padding: 0.5rem; background-color: rgba(220, 240, 220, 0.5); border-bottom-right-radius: 0.5rem; border-bottom-left-radius: 0.5rem; margin-left: auto; margin-right: auto; text-align: center;">
Your feedback can change what we ask on our next survey! Please post suggestions to our Mastodon feed or email them to <a href="mailto:team@uharm.org">team@uharm.org</a>.
</div>

<h3>Why don't we know how often security technologies backfire?</h3>
We started this research because we know far less about the scale of harm we collectively suffer from security technologies backfiring against us than we do about security technologies failing to protect us from others (breaches). For example, we know more about the prevalence with which people's email or social accounts are broken into than the prevalence with which people are permanently locked out of these accounts.

Backfires are rarely reported or measured because, unlike breaches, there is no legislation requiring their reporting. Further, most backfires impact one user at a time, at a scale too small to be newsworthy, and users often blame themselves. Yet, security technologies can cause as much harm when they backfire as when they are breached; they can prevent us from accessing and services at times of critical need, or they can lock us out permanently. The companies that build and deploy security technologies rarely suffer consequences when their products backfire. These companies are reluctant to measure backfires, and are even more reluctant to report their measurements, as doing so would call attention to their product's potential for harm, leading fewer people to use them. The potential for security technology to backfire will not be addressed unless outsiders draw attention it, or compel companies to report incidents of backfiring.

As our research team set out to gauge the prevalence with which security technologies backfire, and the scale of the collective harm that results, we encountered a challenge: we lacked a baseline for the prevalence and harm we collectively suffer from all the ways in which we have come to rely on technology and how it can fail us. And so, we expanded our survey to answer two complementary questions:

<style>
	ol ol { list-style-type: lower-alpha; } and ol ol ol { list-style-type: lower-roman; }
</style>
<ol>
<li>What is the collective harm of technology failing us, being used against us, or otherwise causing harm?</li>
<li>What is the prevalence and collective harm caused by security technologies backfiring...
		<ol>
		<li>as compared to the harms that result from those technologies failing to protect us from breaches, and</li>
		<li>as compared to the broader harms that result from technology failing us, being used against us, or otherwise causing harm?</li>
		</ol>
</li>
</ol>

<h3>Our survey</h3>
We surveyed people to ask about 13 experiences they may or may not have suffered, as well as three harms that need not be attributed to a single specific experience. We asked about both breaches and backfires of the security protecting their (1/2) devices, (3/4) email accounts, (5/6) social media accounts, (7/8) financial accounts, and (9/10) stored passwords (password manager accounts)â€”a total of ten events (five breach experiences each paired with a backfire experience). Another three were experiences that were not specifically security related and did not have breach/backfire pairings: (11) losing data when replacing a device (e.g., upgrading a phone), (12) having technology fail or behave in a way other than what was promised/expected, and (13) having technology used to abuse, harass, or embarrass them. The three harms we asked about that need not be connected to a specific experience were (14) permanently losing photos, (15) permanently losing emails, and (16), experiencing mental distress or health issues connected with their use of technology.

Figure 1 displays the fraction of our *pilot* participants who reported these harmful experiences (1-13) and harms (14-16), broken down by the severity of the resulting harm they experienced (the sub bars).

<figure>
	<img src="/graphs/pilot11/scenario-harm-likert-percent.svg" alt="TBD"/>
	<figcaption>The percent of <i>pilot</i> participants who reported experiencing events causing harms (the 13 bars on the left) or technology-related harms that they did not need to associate with specific events (the 3 bars on the right). Losses due to failures of security measures to protect participants from attack are paired (left bar) against harms due to security measures themselves harming participants (right bar). Each bar is broken down into colors by the Likert severity of harm each participant reported on a Likert scale.</figcaption><b>Figure 1</b>	
</figure>

Not surprisingly, more participants reported experiencing harms than specific scenarios that might have caused harms. Still, far more participants reported that technology had a deleterious effect on their mental health than we expected (even though we had added the question based on the prevalence of mental-health related issues in earlier pilots.) Would participants have associated technology use and mental health concerns if we had not prompted them on a question about mental health?

We can answer this question because we started our survey by asking participants to describe the three most harmful technology-related events they had suffered, without prompting them with specific experiences or harms. When we later asked whether they had suffered the 16 experiences and harms, we would then ask participants whether they had described those experiences/harms amongst those three most-harmful experiences. Indeed, most who reported that technology had impacted their mental health connected this back to one of those three experiences, as shown in Figure 2.

> 

<figure>
  <img src="/graphs/pilot11/scenario-bar-chart.svg" alt="A bar chart summarizing the percent of participants who had experienced each harm scenario."/>
  <figcaption>The percent of <i>pilot</i> participants who reported experiencing events causing harms (the 13 bars on the left) or technology-related harms that they did not need to associate with specific events (the 3 bars on the right). Losses due to failures of security measures to protect participants from attack are paired (left bar) against harms due to security measures themselves harming participants (right bar). Each bar is broken down into colors based on whether the participant connected the experience/harm to one of the three worst experiences they described at the start of the study ("original"), whether they said they should have included the experience/harm as one of their three worst ("revised"), or whether it did not warrant a position in their top three ("not worst"). Those that had suffered the experience/harm are broken down into those who believe the harm could or could not happen to them.</figcaption><b>Figure 2</b>
</figure>

